{"metadata":{"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install nltk","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting nltk\n  Downloading nltk-3.5.zip (1.4 MB)\n\u001b[K     |████████████████████████████████| 1.4 MB 3.0 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: click in /srv/conda/envs/notebook/lib/python3.7/site-packages (from nltk) (7.1.2)\nRequirement already satisfied: joblib in /srv/conda/envs/notebook/lib/python3.7/site-packages (from nltk) (0.17.0)\nCollecting regex\n  Downloading regex-2020.10.23-cp37-cp37m-manylinux2010_x86_64.whl (662 kB)\n\u001b[K     |████████████████████████████████| 662 kB 11.6 MB/s eta 0:00:01\n\u001b[?25hCollecting tqdm\n  Downloading tqdm-4.50.2-py2.py3-none-any.whl (70 kB)\n\u001b[K     |████████████████████████████████| 70 kB 9.7 MB/s  eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: nltk\n  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434676 sha256=dbd279644234fcc068fb9d1645a7442615a253a85a892c496e5583055e699b89\n  Stored in directory: /home/jovyan/.cache/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\nSuccessfully built nltk\nInstalling collected packages: regex, tqdm, nltk\nSuccessfully installed nltk-3.5 regex-2020.10.23 tqdm-4.50.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nnltk.download('wordnet')","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet.zip.\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport csv\nfrom nltk.tokenize import  word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer \n\ndata = pd.read_csv('lyrics-data.csv', header=0, usecols=['SName','Lyric'])\nsname = pd.read_csv('lyrics-data.csv', header=0, usecols=['SName'])\nlyrics = pd.read_csv('lyrics-data.csv', header=0, usecols=['Lyric'])\n\nsong_names = np.asarray(sname)\nlyrics = np.asarray(lyrics)\n\ntokens = []\nstemmed = []\nlemmatized = []\n\nprint('number of songs:', len(data))  \n\nfor i in range(len(data)):\n    sentence = str(lyrics[i])\n    tokenized = word_tokenize(sentence)\n    for i in range(len(tokenized)):\n        count = 0\n        for j in range(len(tokens)):\n            if tokenized[i]==tokens[j]:\n                count += 1\n        if count==0:\n            tokens.append(tokenized[i]) \n        else:\n            continue\n            \nwnl = WordNetLemmatizer()\nps = PorterStemmer()\n\nfor i in range(len(tokens)):  \n    word = tokens[i]\n    lemmatized_word = wnl.lemmatize(word)\n    stemmed_word = ps.stem(word)\n    for i in range(len(lemmatized_word)):\n        count = 0\n        for j in range(len(lemmatized)):\n            if lemmatized_word[i]==lemmatized[j]:\n                count += 1\n        if count==0:\n            lemmatized.append(lemmatized_word[i]) \n        else:\n            continue\n    for i in range(len(stemmed_word)):\n        count = 0\n        for j in range(len(stemmed)):\n            if stemmed_word[i]==stemmed[j]:\n                count += 1\n        if count==0:\n            stemmed.append(stemmed_word[i]) \n        else:\n            continue\n    \nres = [i.strip(\"[]\").split(\", \") for i in lemmatized] \nfile = open('lemmatized_lyrics.csv', 'w+', newline ='')\nwith file:     \n    write = csv.writer(file) \n    write.writerows(res) \n\nres = [i.strip(\"[]\").split(\", \") for i in stemmed] \nfile = open('stemmed_lyrics.csv', 'w+', newline ='')\nwith file:     \n    write = csv.writer(file) \n    write.writerows(res)","metadata":{},"execution_count":null,"outputs":[]}]}